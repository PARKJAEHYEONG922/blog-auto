"""
OpenAI í…ìŠ¤íŠ¸ ìƒì„± API í´ë¼ì´ì–¸íŠ¸
GPT-5, GPT-5-mini, GPT-4o, GPT-4o-mini ë“± ëª¨ë“  OpenAI í…ìŠ¤íŠ¸ ëª¨ë¸ ì§€ì›
Latest Chat Completions API (2025) with new parameters
"""
from typing import Dict, Any, List, Optional, Union
import json

from src.foundation.http_client import default_http_client, rate_limiter_manager
from src.foundation.config import config_manager
from src.foundation.exceptions import OpenAIError, handle_api_exception
from src.foundation.logging import get_logger

logger = get_logger("vendors.openai.text")

# ì¤‘ì•™í™”ëœ AI ëª¨ë¸ ì‹œìŠ¤í…œì—ì„œ ë™ì ìœ¼ë¡œ ë¡œë“œ
def get_supported_models():
    """ì¤‘ì•™ ê´€ë¦¬ë˜ëŠ” OpenAI ëª¨ë¸ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
    from src.foundation.ai_models import AIModelRegistry, AIProvider

    supported_models = {}
    openai_text_models = AIModelRegistry.get_text_models_by_provider(AIProvider.OPENAI)

    for model in openai_text_models:
        supported_models[model.id] = {
            "name": model.display_name,
            "description": model.description,
            "max_tokens": model.max_tokens,
            "context_window": model.context_window
        }

    return supported_models

# ë™ì ìœ¼ë¡œ ì§€ì› ëª¨ë¸ ë¡œë“œ
SUPPORTED_MODELS = get_supported_models()


class OpenAITextClient:
    """OpenAI í…ìŠ¤íŠ¸ ìƒì„± API í´ë¼ì´ì–¸íŠ¸ - Latest Chat Completions API (2025)"""

    def __init__(self):
        self.base_url = "https://api.openai.com/v1"
        self.rate_limiter = rate_limiter_manager.get_limiter("openai_text", 0.5)  # 2ì´ˆë‹¹ 1íšŒ
    
    def _get_headers(self) -> Dict[str, str]:
        """API í˜¸ì¶œìš© í—¤ë” ìƒì„±"""
        api_config = config_manager.load_api_config()
        return {
            'Authorization': f'Bearer {api_config.openai_api_key}',
            'Content-Type': 'application/json'
        }
    
    def _check_config(self) -> bool:
        """API ì„¤ì • í™•ì¸"""
        api_config = config_manager.load_api_config()
        return bool(api_config.openai_api_key)
    
    def get_supported_models(self) -> Dict[str, Dict]:
        """ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return SUPPORTED_MODELS
    
    def get_model_info(self, model: str) -> Optional[Dict]:
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return SUPPORTED_MODELS.get(model)
    
    @handle_api_exception
    def generate_text(self,
                     messages: List[Dict[str, str]],
                     model: str = "gpt-4o",
                     temperature: float = 0.7,
                     max_tokens: Optional[int] = None,
                     reasoning_effort: Optional[str] = None,
                     verbosity: Optional[str] = None) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„± - Latest OpenAI Chat Completions API (2025)

        Args:
            messages: ë©”ì‹œì§€ ëª©ë¡
            model: ì‚¬ìš©í•  OpenAI ëª¨ë¸ (gpt-5, gpt-5-mini, gpt-4o ë“±)
            temperature: ì˜¨ë„ ì„¤ì • (0.0-2.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©)
            reasoning_effort: GPT-5 ì‹œë¦¬ì¦ˆ ì¶”ë¡  ë…¸ë ¥ ìˆ˜ì¤€ ("minimal", "low", "medium", "high")
            verbosity: GPT-5 ì‹œë¦¬ì¦ˆ ì¶œë ¥ ìƒì„¸ë„ (Responses API ì „ìš©, Chat Completionsì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self._check_config():
            raise OpenAIError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ëª¨ë¸ ì§€ì› ì—¬ë¶€ í™•ì¸
        if model not in SUPPORTED_MODELS:
            raise OpenAIError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model}")

        # ëª¨ë¸ë³„ ê¸°ë³¸ max_tokens ì„¤ì • (2025 ì—…ë°ì´íŠ¸)
        if max_tokens is None:
            if model.startswith("gpt-5"):
                # GPT-5 ì‹œë¦¬ì¦ˆëŠ” ë” íš¨ìœ¨ì ì¸ í† í° ì‚¬ìš©
                if model == "gpt-5":
                    max_tokens = 10000  # GPT-5 ìµœëŒ€ ì„±ëŠ¥
                elif model == "gpt-5-mini":
                    max_tokens = 8000   # GPT-5-mini ê· í˜•
                elif model == "gpt-5-nano":
                    max_tokens = 6000   # GPT-5-nano íš¨ìœ¨
                else:
                    max_tokens = 8000   # ê¸°íƒ€ GPT-5 variants
            elif model == "gpt-4o":
                max_tokens = 8000  # ê¸´ ë¸”ë¡œê·¸ ê¸€ ìƒì„±
            elif model == "gpt-4o-mini":
                max_tokens = 6000  # ì¶©ë¶„í•œ ê¸¸ì´ ì§€ì›
            elif model == "gpt-4-turbo":
                max_tokens = 4000
            else:
                max_tokens = 3000

        # ì†ë„ ì œí•œ ì ìš©
        self.rate_limiter.wait()

        # API í˜ì´ë¡œë“œ êµ¬ì„± (Chat Completions API í˜¸í™˜)
        payload = {
            "model": model,
            "messages": messages
        }
        # NOTE: toolsë¥¼ ì‹¤ì œë¡œ ì“°ëŠ” ë³„ë„ ë©”ì„œë“œì—ì„œë§Œ parallel_tool_callsë¥¼ ë¶™ì´ì„¸ìš”.

        # GPT-5 ì‹œë¦¬ì¦ˆëŠ” max_completion_tokens ì‚¬ìš©, ë‹¤ë¥¸ ëª¨ë¸ì€ max_tokens ì‚¬ìš©
        if model.startswith("gpt-5"):
            payload["max_completion_tokens"] = max_tokens
            # GPT-5 ì‹œë¦¬ì¦ˆëŠ” temperature íŒŒë¼ë¯¸í„°ë¥¼ ë³´ë‚´ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’ 1.0 ì‚¬ìš©)
            # Chat Completionsì—ì„œëŠ” reasoning ê°ì²´ë¡œ ì „ë‹¬
            if reasoning_effort:
                payload["reasoning"] = {"effort": reasoning_effort}  # "minimal"|"low"|"medium"|"high"
            # NOTE: verbosityëŠ” Responses API ì¸¡ ê¸°ëŠ¥ ë¦¬í¬íŠ¸ê°€ ìˆì–´
            # Chat Completionsì— ë„£ìœ¼ë©´ 400(Unknown parameter)ì´ ë‚  ìˆ˜ ìˆì–´ ì œê±°í•©ë‹ˆë‹¤.
        else:
            payload["max_tokens"] = max_tokens
            payload["temperature"] = temperature

        headers = self._get_headers()
        url = f"{self.base_url}/chat/completions"

        logger.info(f"OpenAI í…ìŠ¤íŠ¸ ìƒì„± API í˜¸ì¶œ: {model} (max_tokens: {max_tokens})")
        logger.info(f"OpenAI API í˜ì´ë¡œë“œ ìƒì„¸: model={model}, max_tokens={max_tokens}, temperature={temperature}")

        try:
            response = default_http_client.post(url, headers=headers, json=payload)
            
            # HTTP ìƒíƒœ ì½”ë“œ ì²´í¬ (ì‚¬ìš©ì ì¹œí™”ì  ì˜¤ë¥˜ ë©”ì‹œì§€)
            if response.status_code != 200:
                from src.foundation.exceptions import ExceptionMapper
                user_friendly_error = ExceptionMapper.get_user_friendly_message(
                    response.status_code, 
                    f"OpenAI API Error: {response.text}"
                )
                raise OpenAIError(user_friendly_error)
            
            data = response.json()

            if 'error' in data:
                error_msg = data['error'].get('message', 'Unknown error')
                error_type = data['error'].get('type', 'unknown_error')
                logger.error(f"OpenAI API ì—ëŸ¬ [{error_type}]: {error_msg}")
                
                # OpenAI íŠ¹ì • ì˜¤ë¥˜ë„ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜
                user_friendly_error = f"ğŸ¤– OpenAI ì˜¤ë¥˜\n{error_msg}\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                raise OpenAIError(user_friendly_error)

            if 'choices' in data and len(data['choices']) > 0:
                choice = data['choices'][0]
                generated_text = choice['message']['content']
                finish_reason = choice.get('finish_reason', 'unknown')

                # ì‚¬ìš©ëŸ‰ ì •ë³´ ë¡œê¹… (2025 ì—…ë°ì´íŠ¸)
                usage = data.get('usage', {})
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0)
                total_tokens = usage.get('total_tokens', 0)

                logger.info(f"OpenAI í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(generated_text)}ì, finish_reason: {finish_reason}")
                logger.info(f"í† í° ì‚¬ìš©ëŸ‰ - prompt: {prompt_tokens}, completion: {completion_tokens}, total: {total_tokens}")

                # finish_reason ë¶„ì„ ë¡œê¹… (2025 ì—…ë°ì´íŠ¸)
                if finish_reason == 'length':
                    logger.warning(f"OpenAI ì‘ë‹µì´ max_tokens({max_tokens}) ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤. ë” ê¸´ ê¸€ì„ ì›í•œë‹¤ë©´ max_tokensë¥¼ ëŠ˜ë ¤ì£¼ì„¸ìš”.")
                elif finish_reason == 'content_filter':
                    logger.warning("OpenAI ì½˜í…ì¸  í•„í„°ë¡œ ì¸í•´ ìƒì„±ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                elif finish_reason == 'stop':
                    logger.info("OpenAI ì‘ë‹µì´ ìì—°ìŠ¤ëŸ½ê²Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                elif finish_reason == 'tool_calls':
                    logger.info("OpenAIê°€ ë„êµ¬ í˜¸ì¶œë¡œ ì‘ë‹µì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")

                return generated_text
            else:
                raise OpenAIError("ğŸ¤– OpenAIê°€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

        except json.JSONDecodeError as e:
            logger.error(f"OpenAI API ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise OpenAIError(f"ğŸ¤– OpenAI ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\nê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­: {e}")
        except OpenAIError:
            # ì´ë¯¸ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise
        except Exception as e:
            logger.error(f"OpenAI í…ìŠ¤íŠ¸ ìƒì„± API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise OpenAIError(f"ğŸ¤– OpenAI ì—°ê²° ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\në„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ê³  ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\nê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­: {e}")


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
openai_text_client = OpenAITextClient()